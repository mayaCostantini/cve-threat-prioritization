"""
Automated CVE analysis for threat prioritization and impact prediction
"""

import json
import nltk
import os
import pandas as pd
import re
import sqlite3 as sq3
import tempfile
import zipfile

from gensim import corpora
from gensim.models import LdaModel

nltk.download('stopwords')
nltk.download('wordnet')
lemmatizer = nltk.stem.WordNetLemmatizer()

_CVE_PATTERN = r'^\d{4}-\d{4,7}$'
_FULL_CVE_PATTERN = r'^CVE-\d{4}-\d{4,7}$'
_EXPLOITDB_FILENAME = "exploitdb.db"
_DATA_DIR = os.path.join(os.path.dirname(__file__), "data")
_DATASET_ZIP = "cve.csv.zip"
_DATASET_FILE = "cve.csv"


_THREAT_CLASS_PREDICTION_FEATURES = (
    "description_length",
    "references",
    "configurations_length",
    "cvss_score",
    "access_vector_local",
    "access_vector_network",
    "access_complexity_low",
    "access_complexity_medium",
    "access_complexity_high",
    "authentication",
    "confidentiality_impact_complete",
    "confidentiality_impact_partial",
    "confidentiality_impact_none",
    "integrity_impact_none",
    "integrity_impact_complete",
    "integrity_impact_partial",
    "availability_impact_none",
    "availability_impact_complete",
    "availability_impact_partial",
)

_THREAT_CLASS_PREDICTION_CLASSES = (
    "exploit_publication",
    "malware_inclusion",
)


with tempfile.TemporaryDirectory() as temp_dir:
    with zipfile.ZipFile(os.path.join(_DATA_DIR, _DATASET_ZIP)) as zip_file:
        csv_file = zip_file.extractall(temp_dir)
        df = pd.read_csv(os.path.join(temp_dir, _DATASET_FILE))

# Rename "Unnamed: 0" column to cve_id for clarity
df.rename({"Unnamed: 0": "cve_id"}, axis=1, inplace=True)

# Remove CVEs published before 2002
df["pub_date"] = pd.to_datetime(df['pub_date'])

df = df[df["pub_date"] > "2017-01-01 00:00:00"]  # TODO: change to whole dataset for full training
df = df[df["pub_date"] < "2019-01-01 00:00:00"]

df_len = len(df.index)

# Initialize columns for description lengths, number of references, number of configurations and CVSS score and metrics
for feature in _THREAT_CLASS_PREDICTION_FEATURES:
    df[feature] = [0]*df_len

# Initialize columns for threat prediction classes
for class_ in _THREAT_CLASS_PREDICTION_CLASSES:
    df[class_] = [0]*df_len


summaries = df[["cve_id", "summary"]]
stopwords_ = nltk.corpus.stopwords.words("english")

# Remove non alphabetic characters
summaries["summary"] = summaries["summary"].apply(lambda x: ' '.join([word for word in x.split() if word.isalpha()]))

# Remove stop words from descriptions
summaries["summary"] = summaries["summary"].apply(lambda x: ' '.join([word for word in x.split() if word.lower() not in (stopwords_)]))

# Remove URLs from descriptions
summaries["summary"] = summaries["summary"].apply(lambda x: re.sub(r'http\S+', '', x))

# Lemmatize descriptions and format to lowercase
summaries["summary"] = summaries["summary"].apply(lambda x: ' '.join([lemmatizer.lemmatize(word).lower() for word in x.split()]))

# Train an LDA model on the descriptions and extract 30 topics defined by ten keywords each and their probabilities
processed_summaries = [doc.split() for doc in summaries["summary"]]
dictionary = corpora.Dictionary(processed_summaries)
corpus = [dictionary.doc2bow(doc) for doc in processed_summaries]

num_topics = 30
lda_model = LdaModel(corpus, num_topics=num_topics, id2word=dictionary, passes=10, iterations=50)


# # TODO: incorporate topics in the df and look up where the CVE <-> topic information is stored
for idx, topic in lda_model.print_topics(-1):
    print(f'Topic: {idx} \nWords: {topic}\n')


doc_topics = lda_model.get_document_topics(corpus, per_word_topics=False)

count = 10
for doc_id, doc_topic_dist in enumerate(doc_topics):
    if count < 0:
        break
    print(f"Document {doc_id} topic distribution: {doc_topic_dist}")
    count -= 1


###############################
# Threat class predictive model
###############################


# Map description lengths, number of references, number of configurations and CVSS score to CVEs
_NVD_DIR = os.path.join(_DATA_DIR, "nvdcve")

with tempfile.TemporaryDirectory() as temp_dir:
    for root, dirs, files in os.walk(_NVD_DIR):
        for  file_ in files:
            with zipfile.ZipFile(os.path.join(_NVD_DIR, file_)) as zip_file:
                zip_file.extractall(temp_dir)
                filename = " ".join(file_.rsplit('.', maxsplit=1)[:-1])
                with open(os.path.join(temp_dir, filename), "r") as f:
                    data = json.loads(f.read())["CVE_Items"]
                    for cve in data:
                        cve_id = cve["cve"]["CVE_data_meta"]["ID"]
                        references = len(cve["cve"]["references"])
                        description_length = len(cve["cve"]["description"]["description_data"][0]["value"])
                        configurations_length = len(cve["configurations"]["nodes"])
                        base_metric = cve["impact"].get("baseMetricV2")
                        if base_metric is None:
                            cvss_score = 0.0
                        else:
                            cvss_score = base_metric["cvssV2"]["baseScore"]

                            # Encode CVSSv2 categorical metrics as dummy variables

                            # Access vector:
                            access_vector = base_metric["cvssV2"]["accessVector"]
                            if access_vector == "LOCAL":
                                df.loc[df["cve_id"]==cve_id, "access_vector_local"] = 1
                            elif access_vector == "NETWORK":
                                df.loc[df["cve_id"]==cve_id, "access_vector_network"] = 1

                            # Access complexity:
                            access_complexity = base_metric["cvssV2"]["accessComplexity"]
                            if access_complexity == "LOW":
                                df.loc[df["cve_id"]==cve_id, "access_complexity_low"] = 1
                            elif access_complexity == "MEDIUM":
                                df.loc[df["cve_id"]==cve_id, "access_complexity_medium"] = 1
                            elif access_complexity == "HIGH":
                                df.loc[df["cve_id"]==cve_id, "access_complexity_high"] = 1

                            # Authentication:
                            authentication = base_metric["cvssV2"]["authentication"]
                            if authentication == "NONE":
                                df.loc[df["cve_id"]==cve_id, "authentication"] = 0
                            elif authentication == "SINGLE":
                                df.loc[df["cve_id"]==cve_id, "authentication"] = 1
                            
                            # Confidentiality impact:
                            confidentiality_impact = base_metric["cvssV2"]["confidentialityImpact"]
                            if confidentiality_impact == "COMPLETE":
                                df.loc[df["cve_id"]==cve_id, "confidentiality_impact_complete"] = 1
                            elif confidentiality_impact == "NONE":
                                df.loc[df["cve_id"]==cve_id, "confidentiality_impact_none"] = 1
                            elif confidentiality_impact == "PARTIAL":
                                df.loc[df["cve_id"]==cve_id, "confidentiality_impact_partial"] = 1

                            # Integrity impact:
                            integrity_impact = base_metric["cvssV2"]["integrityImpact"]
                            if integrity_impact == "COMPLETE":
                                df.loc[df["cve_id"]==cve_id, "integrity_impact_complete"] = 1
                            elif integrity_impact == "NONE":
                                df.loc[df["cve_id"]==cve_id, "integrity_impact_none"] = 1
                            elif integrity_impact == "PARTIAL":
                                df.loc[df["cve_id"]==cve_id, "integrity_impact_partial"] = 1

                            # Availability impact:
                            availability_impact = base_metric["cvssV2"]["availabilityImpact"]
                            if availability_impact == "COMPLETE":
                                df.loc[df["cve_id"]==cve_id, "availability_impact_complete"] = 1
                            elif availability_impact == "NONE":
                                df.loc[df["cve_id"]==cve_id, "availability_impact_none"] = 1
                            elif availability_impact == "PARTIAL":
                                df.loc[df["cve_id"]==cve_id, "availability_impact_partial"] = 1

                        df.loc[df["cve_id"]==cve_id, "references"] = references
                        df.loc[df["cve_id"]==cve_id, "description_length"] = description_length
                        df.loc[df["cve_id"]==cve_id, "configurations_length"] = configurations_length
                        df.loc[df["cve_id"]==cve_id, "cvss_score"] = cvss_score


df.to_csv(os.path.join(_DATA_DIR, "threat_modeling_features.csv"))


# # Collect data about exploit publication from
# # ExploitDB, Packetstorm and GitHub PoCs and
# # malware inclusion from ClamAV and CTU


# # ExploitDB & Github PoC repository

df = pd.read_csv(os.path.join(_DATA_DIR, "threat_modeling_features.csv"))

dbfile = os.path.join(_DATA_DIR, _EXPLOITDB_FILENAME)
con = sq3.connect(dbfile)
cur = con.cursor()

cur.execute("SELECT CVE FROM exploitdb")
cves = cur.fetchall()
cves = list(filter(lambda x: x[0] is not None and x[0] != 'N/A', cves))
print(cves[:20])
for i, cve in enumerate(cves):
    entry = cve[0]
    if re.match(_CVE_PATTERN, entry):
        df.loc[df["cve_id"]=="CVE-" + entry, "exploit_publication"] = 1
    else:
        if entry.endswith("...") and re.match(_CVE_PATTERN, entry.split("...")[0]):
            entry = entry.split("...")[0]
            df.loc[df["cve_id"]=="CVE-" + entry, "exploit_publication"] = 1
        elif re.match(_FULL_CVE_PATTERN, entry):
            df.loc[df["cve_id"]== entry, "exploit_publication"] = 1


df.to_csv(os.path.join(_DATA_DIR, "threat_modeling_features.csv"))


# # Packetstorm: skipping because DB file or API not found, crawling delay == 100s

