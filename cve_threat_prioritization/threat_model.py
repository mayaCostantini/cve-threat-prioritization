import os
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np
from sklearn.metrics import (
    classification_report, accuracy_score, precision_score, recall_score, f1_score, fbeta_score,
    confusion_matrix
)
import pickle
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import cross_val_score, KFold


def train_or_load_threat_model(_class, x_train, y_train):
    """
    Train or load the threat model for the specified class (i.e., exploit_publication or malware_inclusion).
    The parameters for each model are as reported in the paper.
    """
    model_path = os.path.join("threat_model", f"clf_{_class}.pkl")
    if os.path.exists(model_path):
        with open(model_path, 'rb') as file:
            clf = pickle.load(file)
        print(f"Model for {_class} loaded.")

        return clf

    os.makedirs(os.path.dirname(model_path), exist_ok=True)

    print(f"Training model for {_class}...")
    if _class == "exploit_publication":
        clf = RandomForestClassifier(n_estimators=300, random_state=42, class_weight='balanced',
                                     min_samples_leaf=8, min_samples_split=22, max_depth=30)
        kf = KFold(n_splits=10, shuffle=True, random_state=42)

        cv_scores = cross_val_score(clf, x_train, y_train, cv=kf,
                                    scoring='accuracy')
        print(f"Mean CV Accuracy: {cv_scores.mean():.4f}")

        clf.fit(x_train, y_train)

    elif _class == "malware_inclusion":
        clf = RandomForestClassifier(n_estimators=200, random_state=42, class_weight='balanced',
                                     max_depth=50, min_samples_leaf=6, min_samples_split=16)
        kf = KFold(n_splits=10, shuffle=True, random_state=42)

        cv_scores = cross_val_score(clf, x_train, y_train, cv=kf,
                                    scoring='accuracy')
        print(f"Mean CV Accuracy: {cv_scores.mean():.4f}")

        clf.fit(x_train, y_train)

    else:
        raise ValueError("Invalid class name.")

    with open(model_path, 'wb') as file:
        pickle.dump(clf, file)
    print(f"Model for {_class} saved.")

    return clf


def find_optimized_threshold(y_true, y_proba, max_value=1):
    """
    Threshold moving based on the f2-score.
    from the paper: "a performance metric that optimizes for recall on the minority class, which is suitable for need."
    """
    thresholds = np.linspace(0, max_value, 100 * max_value)
    f2_scores = []

    for threshold in thresholds:
        y_pred = (y_proba[:, 1] >= threshold).astype(int)
        f2 = fbeta_score(y_true, y_pred, beta=2, average='weighted')
        f2_scores.append(f2)

    optimized_threshold = thresholds[np.argmax(f2_scores)]
    return optimized_threshold


def plot_confusion_matrix(y_true, y_pred, class_title):
    cm = confusion_matrix(y_true, y_pred)
    sns.set(style="whitegrid")
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)
    plt.xlabel("Predicted", fontsize=16)
    plt.ylabel("Actual", fontsize=16)
    plt.title(class_title, fontsize=20)
    plt.xticks(fontsize=14)
    plt.yticks(fontsize=14)
    plt.show()
    plt.close()


def model_evaluation(y_test, y_pred, class_title):
    accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred, average='weighted')
    recall = recall_score(y_test, y_pred, average='weighted')
    f1 = f1_score(y_test, y_pred, average='weighted')
    f2 = fbeta_score(y_test, y_pred, beta=2, average='weighted')

    print(f"\nMetrics for {class_title}:")
    print(f"Accuracy: {accuracy:.4f}")
    print(f"Precision: {precision:.4f}")
    print(f"Recall: {recall:.4f}")
    print(f"F1 Score: {f1:.4f}")
    print(f"F2 Score: {f2:.4f}")
    # print("\nDetailed Classification Report:")
    # print(classification_report(y_test, y_pred))
    print("-" * 50)


def calc_and_plot_feature_importance(clf, x, dict_features, categorical_features, class_title):
    feature_importance = pd.DataFrame({'feature': x.columns, 'importance': clf.feature_importances_})
    feature_importance['original_feature'] = feature_importance['feature'].apply(
        lambda x: next((cat for cat in categorical_features if cat in x), x)
    )

    aggregated_importance = feature_importance.groupby('original_feature').agg({'importance': 'sum'}).reset_index()
    aggregated_importance = aggregated_importance.sort_values('importance', ascending=False)

    sns.set(style="whitegrid")
    plt.figure(figsize=(18, 10))
    sns.barplot(x='importance', y='original_feature', data=aggregated_importance, palette="Set2", legend=True)
    plt.xlabel("Importance", fontsize=16)
    plt.ylabel("Feature", fontsize=16)
    plt.title(f"Feature Importance for {class_title}", fontsize=20)
    plt.xlim(0, max(aggregated_importance['importance']) * 1.1)
    labels = []
    for feature in aggregated_importance['original_feature']:
        if feature in dict_features:
            labels.append(dict_features[feature])
        else:
            labels.append(feature)
    plt.yticks(fontsize=10, labels=labels, ticks=np.arange(len(aggregated_importance)), rotation=45)
    plt.xticks(fontsize=14)

    plt.show()
    plt.close()
